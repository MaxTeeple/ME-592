# -*- coding: utf-8 -*-
"""Weather Image Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lrTn5VLD_YS7OI0UTeFyojTIGIfHcE_o

# Weather Image Classification

## Importing necessary Libraries
"""

#!pip install tensorflow-gpu==2.0.0
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import cohen_kappa_score, classification_report, confusion_matrix
import tensorflow as tf
from tensorflow.keras import backend as K
from tensorflow.keras.preprocessing.image import img_to_array
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, Dense, MaxPooling2D, Activation, Flatten
from imutils import paths
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import regularizers
import cv2
import os
from warnings import filterwarnings
from tqdm import tqdm_notebook as tqdm
filterwarnings("ignore")
import PIL
from PIL import Image
import json
import torch
from torch import onnx

"""## Importing the data and EDA"""

'''Uncomment the lines below to generate the data and labels np_arrays'''

# data  = []
# labels = []
# json1_file = open('bdd100k_labels_images_train.json')
# json1_str = json1_file.read()

# for i in range(1000):
#     inputData = json.loads(json1_str)[i]
#     imgName = inputData['name']
#     weatherLabel = inputData['attributes']['weather']
#     fn = os.path.join(os.path.dirname(__file__), 'All_Images','bdd100k','images','100k','train', imgName)
#     image = cv2.imread(fn)
#     image = img_to_array(image)
#     data.append(image)
#     labels.append(weatherLabel)
#     print(i)

# np.save('data_1000', data)
# np.save('labels_1000', labels)


'''Use these two lines below to load the arrays if they already exist'''
data = np.load('data_1000.npy')
labels = np.load('labels_1000.npy')

# Model will not run without at least two samples of each class.
# Here, we change the single 'foggy' image to 'undefined' ensure the model will train
labels[927] = 'undefined'

"""## Label Encoding for Mutliclass classification"""

flabels = pd.Series(labels)
le = LabelEncoder()
le.fit(flabels)
label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))
label_mapping

flabels = le.transform(flabels)
flabels

flables = np.array(flabels)
data = np.array(data, dtype = "float")/255

f = pd.Series(flables)
f.value_counts()

"""## Splitting the data"""

x_train, x_test, y_train, y_test = train_test_split(data, flabels, random_state = 10, test_size = 0.1, stratify= flabels)

f = pd.Series(flabels)
f.unique()

"""## Building the first model"""

def first_model(width, height, depth, classes):
    inputshape = (height,width, depth)
    if K.image_data_format=="channels_first":
        inputshape = (depth, height, width)
    model = Sequential()
    model.add(Conv2D(20,(3,3), input_shape =inputshape))
    model.add(Activation("relu"))
    model.add(MaxPooling2D(pool_size=(2,2), strides= (2,2)))

    model.add(Conv2D(50,(5,5), input_shape =inputshape))
    model.add(Activation("relu"))
    model.add(MaxPooling2D(pool_size=(2,2), strides= (2,2)))
    
    model.add(Flatten())
    model.add(Dense(units = 500, activation = "relu"))
    model.add(Dense(units = 250, activation = "relu"))
    model.add(Dense(units = classes, activation = "softmax"))

    return model

model1 = first_model(width= 1280, height= 720, depth = 3, classes = len(f.unique()))
Epochs = 15
init_lr = 0.001
opt = Adam(lr = init_lr, decay = init_lr/Epochs)
model1.compile(loss = "sparse_categorical_crossentropy", optimizer = opt, metrics = ["accuracy"])
model1.summary()

H = model1.fit(x = x_train, y = y_train, validation_data = (x_test, y_test), epochs= Epochs)
torch.save(H, "torchModel.pt") # https://pytorch.org/tutorials/recipes/recipes/save_load_across_devices.html
try:
    torch.onnx.export(H, x_train, "OnnxModel.onnx")
except:
    print("exception")

"""## Model Evaluation"""

# plot the training and validation accuracy
N = np.arange(0, Epochs)
plt.figure(figsize = [10,8])
plt.plot(N, H.history["accuracy"], label="train_acc")
plt.plot(N, H.history["val_accuracy"], label="val_acc")
plt.title("CNN: Training and Validation Accuracy")
plt.xlabel("Epoch #", weight="bold")
plt.ylabel("Accuracy", weight="bold")
plt.legend()
plt.show()

N = np.arange(0, Epochs)
plt.style.use("ggplot")
plt.figure(figsize = [10,8])
plt.plot(N, H.history["loss"], label="train_loss")
plt.plot(N, H.history["val_loss"], label="val_loss")
plt.title("CNN: Training & Validation Loss")
plt.xlabel("Epoch #", weight="bold")
plt.ylabel("Loss", weight="bold")
plt.legend()
plt.show()

y_pred_prob_model1 = model1.predict(x_test)

y_pred_model1 = []
for i in y_pred_prob_model1:
    y_pred_model1.append(np.argmax(i))

def model_evaluation(y_test, y_pred):
    print("Cohen Kappa Score: ", cohen_kappa_score(y_test, y_pred))
    print("Classification Report--> \n", classification_report(y_true= y_test, y_pred= y_pred))
    cm =confusion_matrix(y_test, y_pred)
    confusion = pd.DataFrame(cm)
    sns.heatmap(confusion, annot = True, fmt = "d")
    plt.xlabel("Predicted Class")
    plt.ylabel("Actual Class")
    plt.tight_layout()
    plt.show()

model_evaluation(y_test = y_test, y_pred = y_pred_model1)

"""## Building the second model"""

# def second_model(width, height, depth, classes):
#     inputshape = (height,width, depth)
#     if K.image_data_format=="channels_first":
#         inputshape = (depth, height, width)
#     model = Sequential()
#     model.add(Conv2D(20,(3,3), input_shape =inputshape))
#     model.add(Conv2D(20,(3,3), input_shape =inputshape))
#     model.add(Activation("relu"))
#     model.add(MaxPooling2D(pool_size=(2,2), strides= (2,2)))

#     model.add(Conv2D(20,(3,3), input_shape =inputshape))
#     model.add(Conv2D(50,(3,3), input_shape =inputshape))
#     model.add(Activation("relu"))
#     model.add(MaxPooling2D(pool_size=(2,2), strides= (2,2)))

#     model.add(Conv2D(20,(2,2), input_shape =inputshape))
#     model.add(Conv2D(50,(2,2), input_shape =inputshape))
#     model.add(Activation("relu"))
#     model.add(MaxPooling2D(pool_size=(2,2), strides= (2,2)))
    
#     model.add(Flatten())
#     model.add(Dense(units = 500, activation = "relu"))
#     model.add(Dense(units = 500, activation = "relu"))
#     model.add(Dense(units = 250, activation = "relu", kernel_regularizer= "l1"))
#     model.add(Dense(units = classes, activation = "softmax"))

#     return model

# model2 = second_model(width= 28, height= 28, depth = 3, classes = len(f.unique()))
# Epochs = 50
# init_lr = 0.001
# opt = Adam(lr = init_lr, decay = init_lr/Epochs)
# model2.compile(loss = "sparse_categorical_crossentropy", optimizer = opt, metrics = ["accuracy"])

# model2.summary()

# H = model2.fit(x = x_train, y = y_train, validation_data = (x_test, y_test), epochs= Epochs)

# """## Model Evaluation"""

# # plot the training and validation accuracy
# N = np.arange(0, Epochs)
# plt.figure(figsize = [10,8])
# plt.plot(N, H.history["accuracy"], label="train_acc")
# plt.plot(N, H.history["val_accuracy"], label="val_acc")
# plt.title("CNN: Training and Validation Accuracy")
# plt.xlabel("Epoch #", weight="bold")
# plt.ylabel("Accuracy", weight="bold")
# plt.legend()
# plt.show()

# N = np.arange(0, Epochs)
# plt.style.use("ggplot")
# plt.figure(figsize = [10,8])
# plt.plot(N, H.history["loss"], label="train_loss")
# plt.plot(N, H.history["val_loss"], label="val_loss")
# plt.title("CNN: Training & Validation Loss")
# plt.xlabel("Epoch #", weight="bold")
# plt.ylabel("Loss", weight="bold")
# plt.legend()
# plt.show()

# y_pred_prob_model2 = model2.predict(x_test)

# y_pred_prob_model2

# y_pred_model2 = []
# for i in y_pred_prob_model1:
#     y_pred_model2.append(np.argmax(i))

# model_evaluation(y_test = y_test, y_pred = y_pred_model2)

